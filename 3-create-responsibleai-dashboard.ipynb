{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443427883
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#conda install azure-common azure-ai-ml==0.1.0b7 mltable==0.1.0b4 azureml_dataprep azureml_dataprep_rslex responsibleai raiwidgets pandas pyarrow shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1675443428204
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1675443443392
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai import RAIInsights\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1675443443684
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azureml.mlflow import register_model\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "#connect to the workspace\n",
    "registry_name = \"azureml\"\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client =  MLClient.from_config(credential=credential)\n",
    "\n",
    "ml_client_registry = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=ml_client.subscription_id,\n",
    "    resource_group_name=ml_client.resource_group_name,\n",
    "    registry_name=registry_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443443943
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "compute_name = \"trainingcompute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443444223
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute: trainingcompute\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"Standard_DS2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443444430
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "rai_hospital_classifier_version_string = '1'\n",
    "version='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443527814
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_base_name}_{model_name_suffix}:1'\n",
    "azureml_model_id = f'azureml:{expected_model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443528126
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_categorical_numerical_data(dataset):\n",
    "    dataset = dataset.drop([target_column], axis = 1)  \n",
    "    categorical = []\n",
    "    for col, value in dataset.iteritems():\n",
    "        if value.dtype == 'object' or value.dtype == 'bool':\n",
    "            categorical.append(col)\n",
    "    numerical = dataset.columns.difference(categorical)\n",
    "    return categorical, numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('data/train_dataset.parquet')\n",
    "test_data = pd.read_parquet('data/test_dataset.parquet')\n",
    "\n",
    "# name of registered datasets\n",
    "training_dataset_filename = 'hospital_train_parquet'\n",
    "testing_dataset_filename = 'hospital_test_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"readmit_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443528330
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns:  ['race', 'gender', 'age', 'discharge_destination', 'max_glu_serum', 'A1Cresult', 'insulin', 'diabetes_Med_prescribe', 'medicare', 'medicaid']\n",
      "numerical field:  Index(['admission_source', 'num_lab_procedures', 'num_medications',\n",
      "       'num_procedures', 'number_diagnoses', 'primary_diagnosis',\n",
      "       'prior_emergency', 'prior_inpatient', 'prior_outpatient',\n",
      "       'time_in_hospital'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# get categorical and numerical fields from training data\n",
    "categorical, numerical = get_categorical_numerical_data(train_data)\n",
    "print(\"categorical columns: \",  categorical)\n",
    "print(\"numerical field: \", numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675443528546
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "label = \"latest\"\n",
    "\n",
    "rai_constructor_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_constructor\", label=label\n",
    ")\n",
    "\n",
    "# We get latest version and use the same version for all components\n",
    "version = rai_constructor_component.version\n",
    "\n",
    "rai_explanation_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_explanation\", version=version\n",
    ")\n",
    "\n",
    "rai_erroranalysis_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_erroranalysis\", version=version\n",
    ")\n",
    "\n",
    "rai_gather_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_gather\", version=version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@dsl.pipeline(\n",
    "        compute=compute_name,\n",
    "        description=\"RAI computation on hospital readmit classification data\",\n",
    "        experiment_name=\n",
    "        f\"RAI_hospital_Classification_RAIInsights_Computation_{model_name_suffix}\",\n",
    "    )\n",
    "def rai_classification_pipeline(\n",
    "        target_column_name,\n",
    "        training_data,\n",
    "        testing_data\n",
    "    ):\n",
    "        # Initiate the RAIInsights\n",
    "        create_rai_job = rai_constructor_component(\n",
    "            title=\"RAI Dashboard\",\n",
    "            task_type=\"classification\",\n",
    "            model_info=expected_model_id,\n",
    "            model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),            \n",
    "            train_dataset=training_data,\n",
    "            test_dataset=testing_data,\n",
    "            target_column_name=target_column_name,\n",
    "            #classes=json.dumps(['not readmitted', 'readmitted']),\n",
    "            categorical_column_names=json.dumps(categorical),\n",
    "        )\n",
    "        create_rai_job.set_limits(timeout=120)\n",
    "        \n",
    "        # Add an explanation\n",
    "        explain_job = rai_explanation_component(\n",
    "            comment=\"Explanation for hospital remitted less than 30days  classification\",\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        explain_job.set_limits(timeout=120)\n",
    "        \n",
    "        # Add error analysis\n",
    "        erroranalysis_job = rai_erroranalysis_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        erroranalysis_job.set_limits(timeout=120)\n",
    "\n",
    "        # Combine everything\n",
    "        rai_gather_job = rai_gather_component(\n",
    "            constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            insight_1=explain_job.outputs.explanation,\n",
    "            insight_4=erroranalysis_job.outputs.error_analysis,\n",
    "        )\n",
    "        rai_gather_job.set_limits(timeout=120)\n",
    "\n",
    "        rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "        rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "        return {\n",
    "            \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "            \"ux_json\": rai_gather_job.outputs.ux_json\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1675436823420
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading rai_hospital_readmission_score_card_config.json\u001b[32m (< 1 MB): 100%|██████████| 528/528 [00:00<00:00, 48.8kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n",
      "Latest status : Running\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from azure.ai.ml import Output\n",
    "\n",
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_classification_pipeline(\n",
    "    target_column_name=target_column,\n",
    "    training_data=hospital_train_parquet,\n",
    "    testing_data=hospital_test_parquet,\n",
    ")\n",
    "\n",
    "# Workaround to enable the download\n",
    "rand_path = str(uuid.uuid4())\n",
    "insights_pipeline_job.outputs.dashboard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.ux_json = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/ux_json/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "\n",
    "\n",
    "# submit pipeline\n",
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
